{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: LoRA (Low Rank Adaptation)\n",
    "* Model: distilbert-base-uncased (for sequence classification)\n",
    "* Evaluation approach: Accuracy using the Hugging Face Trainer\n",
    "* Fine-tuning dataset: is_sarcastic (from hugging face library)\n",
    "\n",
    "### Project: Movie review sentiment analysis.\n",
    "\n",
    "### Project Overview\n",
    "#### Project Introduction:\n",
    "This project demonstrates the process of lightweight fine-tuning using a pre-trained model for sentiment analysis on movie reviews.\n",
    "\n",
    "#### Project Summary:\n",
    "In this project, we will:\n",
    "* Load a pre-trained model and evaluate its performance.\n",
    "* Perform parameter-efficient fine-tuning using the pre-trained model.\n",
    "* Perform inference using the fine-tuned model and compare its performance to the original model.\n",
    "#### Key Concepts:\n",
    "* Sentiment detection requires understanding context and subtle cues.\n",
    "* Using PEFT allows us to fine-tune the model efficiently without needing extensive computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e82cb15f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.5.0)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.36.0)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.18.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/student/.local/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.11.2)\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/student/.local/lib/python3.10/site-packages (from peft) (24.0)\n",
      "Requirement already satisfied: pyyaml in /home/student/.local/lib/python3.10/site-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.2)\n",
      "Requirement already satisfied: tqdm in /home/student/.local/lib/python3.10/site-packages (from peft) (4.66.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.0.1)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from peft) (0.25.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: filelock in /home/student/.local/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/student/.local/lib/python3.10/site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: requests in /home/student/.local/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/student/.local/lib/python3.10/site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (15.0.1)\n",
      "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /home/student/.local/lib/python3.10/site-packages (from datasets) (2024.2.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/student/.local/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/student/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/student/.local/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/student/.local/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/student/.local/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/student/.local/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/student/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/student/.local/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/student/.local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/student/.local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/student/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/student/.local/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.1 threadpoolctl-3.5.0\n"
     ]
    }
   ],
   "source": [
    "! pip install scikit-learn peft transformers datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10039511",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import logging\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments, EvalPrediction\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, AutoPeftModelForSequenceClassification\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b702c8d9",
   "metadata": {},
   "source": [
    "## Loading and Preparing the Dataset\n",
    "In this section, we load the IMDb dataset and prepare it for fine-tuning. We use only a subset of the data to ensure quick experimentation and validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ac7ea61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb67c86a1c1413b8d3ee043bf7f8998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 21.0M/21.0M [00:00<00:00, 22.6MB/s]\n",
      "Downloading data: 100%|██████████| 20.5M/20.5M [00:01<00:00, 19.1MB/s]\n",
      "Downloading data: 100%|██████████| 42.0M/42.0M [00:01<00:00, 27.7MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d3ab99934a490d85aeddca1b91e676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a3a5e1c2cce4cf2939d66ace43206ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dba8758eac448718575eb45a5aabd90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the IMDb dataset and use a smaller subset for testing\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset = imdb_dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "eval_dataset = imdb_dataset[\"test\"].shuffle(seed=42).select(range(200))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100c309a",
   "metadata": {},
   "source": [
    "## Loading the Pre-trained Model\n",
    "Here, we load the pre-trained BERT model (`distilbert-base-uncased`) and its tokenizer. We also configure the model to use the appropriate padding token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44750d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model and tokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36a6ec4",
   "metadata": {},
   "source": [
    "## Tokenizing the Dataset\n",
    "The dataset is tokenized to convert the text into the format required by the BERT model. This includes adding padding and truncating the sequences to a fixed length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7ff55b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Tokenizing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8db48e1d997b4c2c8fcc0847d3222ef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e9a3dbd216c4dc6903fc1e723a7fd62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "logger.info(\"Tokenizing dataset...\")\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set the format for PyTorch\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "eval_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327f00c3",
   "metadata": {},
   "source": [
    "## Evaluating the Pre-trained Model\n",
    "We evaluate the pre-trained BERT model using the tokenized IMDb dataset. We use accuracy as the primary metric for evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb5f687d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Evaluating pretrained model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 2:03:57]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Pretrained model evaluation results: {'eval_loss': 0.6939542889595032, 'eval_accuracy': 0.46, 'eval_f1': 0.4593517406962785, 'eval_precision': 0.4620289855072464, 'eval_recall': 0.46, 'eval_runtime': 44.744, 'eval_samples_per_second': 4.47, 'eval_steps_per_second': 0.559}\n"
     ]
    }
   ],
   "source": [
    "# Define metrics function\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(p.label_ids, preds, average='weighted')\n",
    "    acc = accuracy_score(p.label_ids, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "# Evaluate the pretrained model\n",
    "logger.info(\"Evaluating pretrained model...\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "pretrained_eval_results = trainer.evaluate()\n",
    "logger.info(f\"Pretrained model evaluation results: {pretrained_eval_results}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41ac936",
   "metadata": {},
   "source": [
    "## Creating a PEFT Model\n",
    "We create a parameter-efficient fine-tuning (PEFT) model using LoRA (Low Rank Adaptation) technique.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96142c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Creating PEFT model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,626,628 || all params: 67,989,508 || trainable%: 2.3924691439155583\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Creating PEFT model...\")\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_lin\", \"k_lin\", \"v_lin\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1920192c",
   "metadata": {},
   "source": [
    "## Training the PEFT Model\n",
    "We fine-tune the PEFT model using the tokenized IMDb dataset. The Hugging Face Trainer is used to handle the training process, including batching and optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0247f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [189/189 30:26, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.683581</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>0.551108</td>\n",
       "      <td>0.693588</td>\n",
       "      <td>0.610000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.684900</td>\n",
       "      <td>0.676295</td>\n",
       "      <td>0.635000</td>\n",
       "      <td>0.585047</td>\n",
       "      <td>0.725757</td>\n",
       "      <td>0.635000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.684900</td>\n",
       "      <td>0.673660</td>\n",
       "      <td>0.665000</td>\n",
       "      <td>0.645356</td>\n",
       "      <td>0.696763</td>\n",
       "      <td>0.665000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Saving the model...\n"
     ]
    }
   ],
   "source": [
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "# Initialize Trainer for PEFT model\n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the PEFT model\n",
    "logger.info(\"Starting training...\")\n",
    "peft_trainer.train()\n",
    "\n",
    "# Save the PEFT model\n",
    "logger.info(\"Saving the model...\")\n",
    "peft_model.save_pretrained(\"./peft_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54e7c5c",
   "metadata": {},
   "source": [
    "## Loading the Saved PEFT Model\n",
    "We load the saved PEFT model to perform inference and further evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd48f30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "fine_tuned_model = AutoPeftModelForSequenceClassification.from_pretrained(\"./peft_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e65a37d",
   "metadata": {},
   "source": [
    "## Evaluating the Fine-Tuned Model\n",
    "We evaluate the fine-tuned model using the same metrics and dataset to compare its performance to the original model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e6474e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Evaluating the fine-tuned model...\n",
      "INFO:__main__:Evaluation results: {'eval_loss': 0.6736596822738647, 'eval_accuracy': 0.665, 'eval_f1': 0.6453562929490093, 'eval_precision': 0.6967629315877295, 'eval_recall': 0.665, 'eval_runtime': 48.5853, 'eval_samples_per_second': 4.116, 'eval_steps_per_second': 0.515}\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Evaluating the fine-tuned model...\")\n",
    "eval_results = trainer.evaluate()\n",
    "logger.info(f\"Evaluation results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2810e7d5",
   "metadata": {},
   "source": [
    "## Generating and Reviewing Predictions\n",
    "To validate the model's performance, we generate predictions for a few samples from the test dataset and manually compare them with the true labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "681674a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Generating predictions for sample data...\n",
      "INFO:__main__:Sample 1:\n",
      "INFO:__main__:Text: sex, drugs, racism and of course you abc's. what more could you want in a kid's show! < br / > < br / > - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ...\n",
      "INFO:__main__:True Label: 1, Predicted Label: 0\n",
      "\n",
      "INFO:__main__:True Sentiment: Positive\n",
      "INFO:__main__:Predicted Sentiment: Negative\n",
      "\n",
      "INFO:__main__:Sample 2:\n",
      "INFO:__main__:Text: jessica bohl plays daphne, the sexually precocious suburban teenager struggling with the hell of high school. daphne's neighbor is buddy ( richard brundage ), a depressed middle - aged man still angry...\n",
      "INFO:__main__:True Label: 1, Predicted Label: 1\n",
      "\n",
      "INFO:__main__:True Sentiment: Positive\n",
      "INFO:__main__:Predicted Sentiment: Positive\n",
      "\n",
      "INFO:__main__:Sample 3:\n",
      "INFO:__main__:Text: this movie was absolutely one of the worst movies i have ever seen. the plot could have been made to work, had the movie been written better. the acting was some of the worst i have ever seen. i was v...\n",
      "INFO:__main__:True Label: 0, Predicted Label: 0\n",
      "\n",
      "INFO:__main__:True Sentiment: Negative\n",
      "INFO:__main__:Predicted Sentiment: Negative\n",
      "\n",
      "INFO:__main__:Sample 4:\n",
      "INFO:__main__:Text: this is a really sad, and touching movie! it deals with the subject of child abuse. it's really sad, but mostly a true story, because it happens everyday. elijah wood and joseph mazzello play the two ...\n",
      "INFO:__main__:True Label: 1, Predicted Label: 1\n",
      "\n",
      "INFO:__main__:True Sentiment: Positive\n",
      "INFO:__main__:Predicted Sentiment: Positive\n",
      "\n",
      "INFO:__main__:Sample 5:\n",
      "INFO:__main__:Text: get smart should be titled get stupid. there is not one funny line or gag in the entire film. this film is so bad it makes the austin powers films look shakespearean. a few more films like this and st...\n",
      "INFO:__main__:True Label: 0, Predicted Label: 0\n",
      "\n",
      "INFO:__main__:True Sentiment: Negative\n",
      "INFO:__main__:Predicted Sentiment: Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate and review predictions\n",
    "logger.info(\"Generating predictions for sample data...\")\n",
    "sample_indices = random.sample(range(len(eval_dataset)), 5)\n",
    "samples = [eval_dataset[i] for i in sample_indices]\n",
    "\n",
    "for idx, sample in enumerate(samples):\n",
    "    # Prepare inputs\n",
    "    inputs = {\n",
    "        'input_ids': sample['input_ids'].unsqueeze(0),\n",
    "        'attention_mask': sample['attention_mask'].unsqueeze(0)\n",
    "    }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = fine_tuned_model(**inputs)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    predicted_label = logits.argmax(-1).item()\n",
    "    true_label = sample['label'].item()\n",
    "\n",
    "    # Decode the input_ids to get the original text\n",
    "    original_text = tokenizer.decode(sample['input_ids'], skip_special_tokens=True)\n",
    "\n",
    "    logger.info(f\"Sample {idx + 1}:\")\n",
    "    logger.info(f\"Text: {original_text[:200]}...\")\n",
    "    logger.info(f\"True Label: {true_label}, Predicted Label: {predicted_label}\\n\")\n",
    "\n",
    "    # Print label meanings\n",
    "    label_meanings = {0: \"Negative\", 1: \"Positive\"}\n",
    "    logger.info(f\"True Sentiment: {label_meanings[true_label]}\")\n",
    "    logger.info(f\"Predicted Sentiment: {label_meanings[predicted_label]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0227fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
